{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data preprocessing and Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe and import necessary liabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features with more than 20% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 50))\n",
    "sns.barplot(x=data.isnull().sum(), y=data.columns)\n",
    "plt.xlabel('Missing Values')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "for col in data.columns:\n",
    "    if data[col].isnull().sum() > 103558:\n",
    "        data = data.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features with the same value for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"disbursement_method\", \"hardship_flag\"]\n",
    "for col in cols:\n",
    "    data = data.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding using one hot encoding and frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_en_cols = [\"term\", \"initial_list_status\", \"debt_settlement_flag\"]   \n",
    "data_encoded = pd.get_dummies(data, columns=oh_en_cols)\n",
    "\n",
    "f_en_cols = [\"application_type\", \"home_ownership\", \"verification_status\", \"grade\", \"emp_length\", \"purpose\", \"title\", \n",
    "         \"emp_title\", \"issue_d\", \"zip_code\", \"addr_state\", \"earliest_cr_line\", \"sub_grade\", \"last_pymnt_d\", \n",
    "         \n",
    "         \"last_credit_pull_d\", \"pymnt_plan\"]\n",
    "for col in f_en_cols:\n",
    "    dr_frequency_map=data_encoded[col].value_counts().to_dict()\n",
    "    data_encoded[col] = data_encoded[col].map(dr_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values of features with a skewed distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Identify skewed columns\n",
    "numeric_cols = data_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "skewed_cols = data_encoded[numeric_cols].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "# Choose a skewness threshold \n",
    "skew_threshold = 0.5\n",
    "skewed_cols = skewed_cols[abs(skewed_cols) > skew_threshold].index\n",
    "\n",
    "# Replace missing values with the median in skewed columns\n",
    "for col in skewed_cols:\n",
    "    median_value = data_encoded[col].median()\n",
    "    data_encoded[col].fillna(median_value, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the values of features (features with relatively low missing values) with the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 50))\n",
    "sns.barplot(x=data_encoded.isnull().sum(), y=data_encoded.columns)\n",
    "plt.xlabel('Missing Values')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "fill_mean = [\"bc_util\", \"revol_util\", \"mo_sin_old_il_acct\", \"percent_bc_gt_75\"]\n",
    "\n",
    "for col in fill_mean:\n",
    "    data_encoded[col].fillna(data_encoded[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill other features with missing values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = data_encoded.fillna(value=0)\n",
    "plt.figure(figsize=(20, 50))\n",
    "sns.barplot(x=data_encoded.isnull().sum(), y=data_encoded.columns)\n",
    "plt.xlabel('Missing Values')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap to see the correlations between a group of features and the loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['installment', 'int_rate', 'num_sats', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\n",
    "        'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', \"mo_sin_rcnt_rev_tl_op\", \"num_rev_tl_bal_gt_0\",\n",
    "        \"num_rev_accts\", \"num_bc_tl\", \"num_bc_sats\", \"num_actv_rev_tl\", \"num_actv_bc_tl\", \"mort_acc\", \n",
    "        \"num_accts_ever_120_pd\", \"mths_since_recent_inq\", \"mths_since_recent_bc\", \n",
    "        \"bc_open_to_buy\", \"mo_sin_rcnt_tl\", \"mo_sin_old_rev_tl_op\", \"mo_sin_old_il_acct\", \n",
    "        \"total_rev_hi_lim\", \"total_rec_late_fee\", \"total_rec_int\" , \"tot_cur_bal\", \"tot_coll_amt\", \n",
    "        \"collections_12_mths_ex_med\", \"policy_code\", \"avg_cur_bal\",\"issue_d\", \"earliest_cr_line\", \n",
    "        \"last_pymnt_d\", \"last_credit_pull_d\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \n",
    "        \"total_il_high_credit_limit\", \"loan_status\"]\n",
    "\n",
    "corr_matrix = data_encoded[cols].corr()\n",
    "plt.figure(figsize=(50, 50))\n",
    "heatmap = sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features with weak correaltion with loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['installment', 'int_rate', 'num_sats', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', \n",
    "               'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', \"mo_sin_rcnt_rev_tl_op\", \"num_rev_tl_bal_gt_0\", \n",
    "               \"num_rev_accts\", \"num_bc_tl\", \"num_bc_sats\", \"num_actv_rev_tl\", \"num_actv_bc_tl\", \"mort_acc\", \n",
    "               \"num_accts_ever_120_pd\", \"mths_since_recent_inq\", \"mths_since_recent_bc\", \"bc_open_to_buy\", \n",
    "               \"mo_sin_rcnt_tl\", \"mo_sin_old_rev_tl_op\", \"mo_sin_old_il_acct\", \"total_rev_hi_lim\", \"total_rec_late_fee\", \n",
    "               \"total_rec_int\", \"total_rev_hi_lim\", \"tot_cur_bal\", \"tot_coll_amt\", \"collections_12_mths_ex_med\", \n",
    "               \"policy_code\", \"avg_cur_bal\",\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\", \n",
    "               \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \"total_il_high_credit_limit\"]\n",
    "data_encoded = data_encoded.drop(col_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting loan status from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_encoded[\"loan_status\"]\n",
    "data_encoded = data_encoded.drop(\"loan_status\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(data_encoded)\n",
    "scaled_data=scaler.transform(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=20,random_state=42)\n",
    "pca.fit(scaled_data)\n",
    "X_pca=pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fit model to training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with the model and calculating the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_pca)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(data_encoded.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "!pip install shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer=shap.Explainer(model)\n",
    "shap_values=explainer(X_pca)\n",
    "shap.plots.waterfall(shap_values[0])\n",
    "shap.plots.bar(shap_values)\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe and import necessary liabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features with more than 20% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].isnull().sum() > 34520:\n",
    "        data = data.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features with the same value for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"disbursement_method\", \"hardship_flag\"]\n",
    "for col in cols:\n",
    "    data = data.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding using one hot encoding and frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_en_cols = [\"term\", \"initial_list_status\", \"debt_settlement_flag\"]   \n",
    "data_encoded = pd.get_dummies(data, columns=oh_en_cols)\n",
    "\n",
    "f_en_cols = [\"application_type\", \"home_ownership\", \"verification_status\", \"grade\", \"emp_length\", \"purpose\", \"title\", \n",
    "         \"emp_title\", \"issue_d\", \"zip_code\", \"addr_state\", \"earliest_cr_line\", \"sub_grade\", \"last_pymnt_d\", \n",
    "         \"last_credit_pull_d\", \"pymnt_plan\"]\n",
    "for col in f_en_cols:\n",
    "    dr_frequency_map=data_encoded[col].value_counts().to_dict()\n",
    "    data_encoded[col] = data_encoded[col].map(dr_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values of features with a skewed distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# Identify skewed columns\n",
    "numeric_cols = data_encoded.select_dtypes(include=['float64', 'int64']).columns\n",
    "skewed_cols = data_encoded[numeric_cols].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "# Choose a skewness threshold \n",
    "skew_threshold = 0.5\n",
    "skewed_cols = skewed_cols[abs(skewed_cols) > skew_threshold].index\n",
    "\n",
    "# Replace missing values with the median in skewed columns\n",
    "for col in skewed_cols:\n",
    "    median_value = data_encoded[col].median()\n",
    "    data_encoded[col].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the values of features (features with relatively low missing values) with the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 50))\n",
    "sns.barplot(x=data_encoded.isnull().sum(), y=data_encoded.columns)\n",
    "plt.xlabel('Missing Values')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n",
    "fill_mean = [\"bc_util\", \"revol_util\", \"mo_sin_old_il_acct\", \"percent_bc_gt_75\"]\n",
    "\n",
    "for col in fill_mean:\n",
    "    data_encoded[col].fillna(data_encoded[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill other features with missing values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = data_encoded.fillna(value=0)\n",
    "plt.figure(figsize=(20, 50))\n",
    "sns.barplot(x=data_encoded.isnull().sum(), y=data_encoded.columns)\n",
    "plt.xlabel('Missing Values')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features with weak correaltion with loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['installment', 'int_rate', 'num_sats', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', \n",
    "               'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', \"mo_sin_rcnt_rev_tl_op\", \"num_rev_tl_bal_gt_0\", \n",
    "               \"num_rev_accts\", \"num_bc_tl\", \"num_bc_sats\", \"num_actv_rev_tl\", \"num_actv_bc_tl\", \"mort_acc\", \n",
    "               \"num_accts_ever_120_pd\", \"mths_since_recent_inq\", \"mths_since_recent_bc\", \"bc_open_to_buy\", \n",
    "               \"mo_sin_rcnt_tl\", \"mo_sin_old_rev_tl_op\", \"mo_sin_old_il_acct\", \"total_rev_hi_lim\", \"total_rec_late_fee\", \n",
    "               \"total_rec_int\", \"total_rev_hi_lim\", \"tot_cur_bal\", \"tot_coll_amt\", \"collections_12_mths_ex_med\", \n",
    "               \"policy_code\", \"avg_cur_bal\",\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\", \n",
    "               \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \"total_il_high_credit_limit\"]\n",
    "data_encoded = data_encoded.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(data_encoded)\n",
    "scaled_data=scaler.transform(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca=PCA(n_components=20,random_state=42)\n",
    "pca.fit(scaled_data)\n",
    "X_pca=pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the modified csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_df = pd.DataFrame(data_encoded)\n",
    "final_result_df.insert(0, 'loan_status', y_pred)\n",
    "final_result_df.to_csv('210099V.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer=shap.Explainer(model)\n",
    "shap_values=explainer(X_pca)\n",
    "shap.plots.waterfall(shap_values[0])\n",
    "shap.plots.bar(shap_values)\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
